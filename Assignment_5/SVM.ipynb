{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pos_train_data = pd.read_csv('train_pos.csv',sep = ',')\n",
    "neg_train_data = pd.read_csv('train_neg.csv',sep = ',')\n",
    "pos_test_data = pd.read_csv('test_pos.csv',sep = ',')\n",
    "neg_test_data = pd.read_csv('test_neg.csv',sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_data=pos_train_data[['Text','Label']]\n",
    "neg_train_data=neg_train_data[['Text','Label']]\n",
    "pos_test_data=pos_test_data[['Text','Label']]\n",
    "neg_test_data=neg_test_data[['Text','Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police Story is one of Jackie Chan's classic f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marlon Brando had long since lost interest in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am Black American and I loved this soap oper...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have never seen a worse movie.&lt;br /&gt;&lt;br /&gt;It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paul Reiser did a spectacular job in writing t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  Police Story is one of Jackie Chan's classic f...      1\n",
       "1  Marlon Brando had long since lost interest in ...      0\n",
       "2  I am Black American and I loved this soap oper...      1\n",
       "3  I have never seen a worse movie.<br /><br />It...      0\n",
       "4  Paul Reiser did a spectacular job in writing t...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train=pd.concat([pos_train_data,neg_train_data],ignore_index = True)\n",
    "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I remember seeing this film when I was 13 year...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hitchcock once gave an interview where he said...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A formulaic story with all the tired cliches. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is one of those films with a great potent...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm not sure I've ever seen a film as bad as t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  I remember seeing this film when I was 13 year...      1\n",
       "1  Hitchcock once gave an interview where he said...      1\n",
       "2  A formulaic story with all the tired cliches. ...      0\n",
       "3  This is one of those films with a great potent...      0\n",
       "4  I'm not sure I've ever seen a film as bad as t...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.concat([pos_test_data,neg_test_data],ignore_index = True)\n",
    "data_test = data_test.sample(frac=1).reset_index(drop=True)\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "table = str.maketrans('','',punctuation)\n",
    "\n",
    "def textclean(text):\n",
    "    tokens=word_tokenize(text, language=\"english\")\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    tokens=[w.translate(table) for w in tokens]\n",
    "    tokens=[word for word in tokens if not word in stop_words]\n",
    "    token = [word for word in tokens if len(word)>1 ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=[]\n",
    "for index,row in data_train.iterrows():\n",
    "    text=row['Text'].lower()\n",
    "    reviews.append(textclean(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['police',\n",
       " 'story',\n",
       " 'one',\n",
       " 'jackie',\n",
       " 'chan',\n",
       " 'classic',\n",
       " 'films',\n",
       " 'helped',\n",
       " 'shape',\n",
       " 'hong',\n",
       " 'kong',\n",
       " 'cinema',\n",
       " 'masterpiece',\n",
       " 'missed',\n",
       " 'action',\n",
       " 'movie',\n",
       " 'fan',\n",
       " 'beginning',\n",
       " 'obvious',\n",
       " 'jackie',\n",
       " 'chan',\n",
       " 'stunt',\n",
       " 'team',\n",
       " 'literally',\n",
       " 'risked',\n",
       " 'live',\n",
       " 'make',\n",
       " 'film',\n",
       " 'action',\n",
       " 'stunts',\n",
       " 'extremely',\n",
       " 'realistic',\n",
       " 'innovative',\n",
       " 'even',\n",
       " 'today',\n",
       " 'movie',\n",
       " 'outdone',\n",
       " 'police',\n",
       " 'story',\n",
       " 'dangerous',\n",
       " 'stunts',\n",
       " 'many',\n",
       " 'people',\n",
       " 'hospitalized',\n",
       " 'police',\n",
       " 'story',\n",
       " 'including',\n",
       " 'jackie',\n",
       " 'chan',\n",
       " 'fighting',\n",
       " 'indisputably',\n",
       " 'exceptional',\n",
       " 'stunts',\n",
       " 'fighting',\n",
       " 'movie',\n",
       " 'helped',\n",
       " 'change',\n",
       " 'define',\n",
       " 'jackie',\n",
       " 'chan',\n",
       " 'use',\n",
       " 'props',\n",
       " 'throughout',\n",
       " 'film',\n",
       " 'chan',\n",
       " 'uses',\n",
       " 'odd',\n",
       " 'object',\n",
       " 'stop',\n",
       " 'attackers',\n",
       " 'constantly',\n",
       " 'throwing',\n",
       " 'assailants',\n",
       " 'thick',\n",
       " 'glass',\n",
       " 'action',\n",
       " 'feels',\n",
       " 'real',\n",
       " 'stuntmen',\n",
       " 'giving',\n",
       " 'movie',\n",
       " 'give',\n",
       " 'jackie',\n",
       " 'chan',\n",
       " 'coordination',\n",
       " 'br',\n",
       " 'br',\n",
       " 'rest',\n",
       " 'aspects',\n",
       " 'film',\n",
       " 'without',\n",
       " 'flaws',\n",
       " 'disappoint',\n",
       " 'action',\n",
       " 'fan',\n",
       " 'chan',\n",
       " 'plays',\n",
       " 'believable',\n",
       " 'risk',\n",
       " 'taking',\n",
       " 'cop',\n",
       " 'shows',\n",
       " 'powerful',\n",
       " 'changes',\n",
       " 'character',\n",
       " 'goes',\n",
       " 'falls',\n",
       " 'escalating',\n",
       " 'desperation',\n",
       " 'plot',\n",
       " 'powerful',\n",
       " 'modern',\n",
       " 'viewer',\n",
       " 'may',\n",
       " 'find',\n",
       " 'tedious',\n",
       " 'times',\n",
       " 'comedy',\n",
       " 'provide',\n",
       " 'good',\n",
       " 'number',\n",
       " 'laughs',\n",
       " 'always',\n",
       " 'distract',\n",
       " 'audience',\n",
       " 'lack',\n",
       " 'action',\n",
       " 'however',\n",
       " 'time',\n",
       " 'period',\n",
       " 'made',\n",
       " 'driving',\n",
       " 'aspects',\n",
       " 'plot',\n",
       " 'entertaining',\n",
       " 'good',\n",
       " 'number',\n",
       " 'interesting',\n",
       " 'well',\n",
       " 'played',\n",
       " 'characters',\n",
       " 'dispersed',\n",
       " 'throughout',\n",
       " 'film',\n",
       " 'br',\n",
       " 'br',\n",
       " 'overall',\n",
       " 'police',\n",
       " 'story',\n",
       " 'without',\n",
       " 'doubt',\n",
       " 'one',\n",
       " 'best',\n",
       " 'action',\n",
       " 'movies',\n",
       " 'ever',\n",
       " 'made',\n",
       " 'even',\n",
       " 'hollywood',\n",
       " 'influences',\n",
       " 'one',\n",
       " 'film',\n",
       " 'ignored']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp39-cp39-win_amd64.whl (23.9 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\anaconda\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.1.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in d:\\anaconda\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Collecting Cython==0.29.28\n",
      "  Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.24\n",
      "    Uninstalling Cython-0.29.24:\n",
      "      Successfully uninstalled Cython-0.29.24\n",
      "Successfully installed Cython-0.29.28 gensim-4.2.0 smart-open-6.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting word2vec\n",
      "  Downloading word2vec-0.11.1.tar.gz (42 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from word2vec) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.2 in d:\\anaconda\\lib\\site-packages (from word2vec) (1.20.3)\n",
      "Building wheels for collected packages: word2vec\n",
      "  Building wheel for word2vec (PEP 517): started\n",
      "  Building wheel for word2vec (PEP 517): finished with status 'done'\n",
      "  Created wheel for word2vec: filename=word2vec-0.11.1-py2.py3-none-any.whl size=181934 sha256=be6cbdea1ba09cf9ce9bd256650c0369bdb6f3812072c52965adfece06c1a27e\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\d7\\3e\\81\\4f4daaa6d86f5b9301c16cb1051baa2cbf8f0a309063862b2b\n",
      "Successfully built word2vec\n",
      "Installing collected packages: word2vec\n",
      "Successfully installed word2vec-0.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "n_dim=200\n",
    "\n",
    "w2v_model =Word2Vec(reviews,min_count=5,vector_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer  = TfidfVectorizer(min_df=10,analyzer=lambda x: x)\n",
    "\n",
    "matrix=vectorizer.fit_transform(reviews)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = dict(zip(vectorizer.get_feature_names(),vectorizer.idf_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_vector(l,size):\n",
    "    vector = np.zeros(size).reshape((1,size))\n",
    "    count = 0.\n",
    "    for word in l:\n",
    "        try:\n",
    "            vector += w2v_model.wv[word].reshape((1, size)) * tfidf[word]\n",
    "            count+=1\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "    if count!=0:\n",
    "        vector /= count\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(data_train)):\n",
    "    converted_review = create_word_vector(reviews[i],n_dim)\n",
    "    #print(converted_review)\n",
    "    X_train.append(np.array(converted_review))\n",
    "    y_train.append(data_train['Label'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "X_train = np.concatenate(X_train)\n",
    "X_train = scale(X_train)\n",
    "y_train = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.concat([pos_test_data,neg_test_data],ignore_index = True)\n",
    "data_test = data_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "validation_reviews = []\n",
    "\n",
    "for index,row in data_test.iterrows():\n",
    "    text = (row['Text'].lower())\n",
    "    validation_reviews.append(textclean(text))\n",
    "    \n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for i in range(len(data_test)):\n",
    "    converted_review = create_word_vector(validation_reviews[i],n_dim)\n",
    "    X_val.append(converted_review)\n",
    "    y_val.append(data_test['Label'][i])\n",
    "        \n",
    "X_val = np.concatenate(X_val)\n",
    "X_val = scale(X_val)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "clf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(X_val[4].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This movie sucked. The problem was not with the cast. I think the cast was great, lots of good talent, lots of great acting. But the script was TERRIBLE! It seemed to be mostly just a frame work in which Steve Carrell could do his improv. And that is what he does best, but it just didn't work here. The script was hard to follow, the story was non-sensical, and scenes were random and lacked direction. Also, much of the action was extremely contrived and poorly thought out. It was a good effort, but as Max says, they missed it by THAT MUCH! I am shocked to see how many glowing reviews there are for this stinker here on the IMDb. Obviously, the movie producers are getting people to write lots of positive reviews on their movies and fill up the entries on the IMDb. If you read the positive reviews and compare them to the negative reviews, it is pretty clear which ones are genuine reviews from normal users.<br /><br />This movie was full of problems and jokes that just didn't work. I loved Steve Carrell in Anchor Man, and I like his comedy and style. But I will tell you that I never once laughed while I was watching this movie. Yes, I had a couple of light moments, a couple of chuckles, but no real laughs. Nothing that struck me at all.<br /><br />Spoiler Alert! One ridiculous scene was when Max had his hands binded on the airplane and he goes to the bathroom to try to escape. He uses his special Swiss Army knife...but instead of just using THE BLADE OF THE KNIFE, he tries to SHOOT the binding with his miniature crossbow. And as the crossbow miss-fires and shoots little arrows into him over and over again (almost putting out his EYE), Max doesn't give up or try the blade instead...no, he just keeps shooting himself with the crossbow. What was he really expecting to do with that crossbow? It seemed to be THE WORST option on the knife to try to remove the bindings. It just made absolutely no sense.<br /><br />That is a good example of the typical circumstances in the scenes that made up this movie. They were ridiculous, poorly thought out, poorly motivated, and made of pure nonsense. And that was truly distracting.<br /><br />As I said, this movie was a big let-down, and I recommend you avoid it. A note to the IMDb: You should do something about these phony reviews that people are leaving. It degrades the authenticity of the site.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['Text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85456\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_val,clf.predict(X_val)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
